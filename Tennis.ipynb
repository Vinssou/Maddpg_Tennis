{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:unityagents:\n'Academy' started successfully!\nUnity Academy name: Academy\n        Number of Brains: 1\n        Number of External Brains : 1\n        Lesson number : 0\n        Reset Parameters :\n\t\t\nUnity brain name: TennisBrain\n        Number of Visual Observations (per agent): 0\n        Vector Observation space type: continuous\n        Vector Observation space size (per agent): 8\n        Number of stacked Vector Observation: 3\n        Vector Action space type: continuous\n        Vector Action space size (per agent): 2\n        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis_Windows_x86_64/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of agents: 2\nSize of each action: 2\nThere are 2 agents. Each observes a state with length: 24\nThe state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n  0.          0.          0.          0.          0.          0.\n  0.          0.          0.          0.         -6.65278625 -1.5\n -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from maddpg import MaddpgAgent\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def evaluate(agent, env):\n",
    "    \n",
    "    gamma=1.0\n",
    "    max_t=5000\n",
    "\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    episode_return = np.zeros(num_agents)\n",
    "    for t in range(max_t):\n",
    "        actions = agent.act(states)\n",
    "        actions = np.array(actions)\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "        agent.step(states, actions, rewards, next_states, dones)\n",
    "        episode_return = episode_return + np.array(rewards) * math.pow(gamma, t)\n",
    "        states = next_states\n",
    "        if np.array(dones).any():\n",
    "            break\n",
    "\n",
    "    return episode_return\n",
    "\n",
    "\n",
    "def train(n_iterations=100000, gamma=1.0, print_every=100):\n",
    "   \n",
    "    np.random.seed(101)\n",
    "\n",
    "    agent = MaddpgAgent(num_agents, state_size, action_size, 10)\n",
    "\n",
    "    print(\"Agent count: \", num_agents)\n",
    "\n",
    "\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "   \n",
    "    for i_iteration in range(1, n_iterations+1):\n",
    "        \n",
    "        rewards = evaluate(agent, env)       \n",
    "        reward = rewards.mean()\n",
    "        scores_deque.append(reward)\n",
    "        avg_score = np.mean(scores_deque)\n",
    "        scores.append(avg_score)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if i_iteration % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}   Exploration: {:.2f}'.format(i_iteration,avg_score, agent.exploration))\n",
    "            \n",
    "        if avg_score >= 0.5:\n",
    "            agent.save()\n",
    "            print('\\nEnvironment solved in {:d} iterations!\\tAverage Score: {:.2f}'.format(i_iteration-100, np.mean(scores_deque)))\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "e Score: -0.00   Exploration: 0.23\n",
      "Episode 50900\tAverage Score: -0.00   Exploration: 0.23\n",
      "Episode 51000\tAverage Score: -0.00   Exploration: 0.23\n",
      "Episode 51100\tAverage Score: -0.00   Exploration: 0.23\n",
      "Episode 51200\tAverage Score: -0.00   Exploration: 0.23\n",
      "Episode 51300\tAverage Score: -0.00   Exploration: 0.23\n",
      "Episode 51400\tAverage Score: -0.00   Exploration: 0.23\n",
      "Episode 51500\tAverage Score: -0.00   Exploration: 0.23\n",
      "Episode 51600\tAverage Score: -0.00   Exploration: 0.23\n",
      "Episode 51700\tAverage Score: -0.00   Exploration: 0.23\n",
      "Episode 51800\tAverage Score: -0.00   Exploration: 0.23\n",
      "Episode 51900\tAverage Score: -0.00   Exploration: 0.23\n",
      "Episode 52000\tAverage Score: -0.00   Exploration: 0.23\n",
      "Episode 52100\tAverage Score: -0.00   Exploration: 0.22\n",
      "Episode 52200\tAverage Score: -0.00   Exploration: 0.22\n",
      "Episode 52300\tAverage Score: -0.00   Exploration: 0.22\n",
      "Episode 52400\tAverage Score: -0.00   Exploration: 0.22\n",
      "Episode 52500\tAverage Score: -0.00   Exploration: 0.22\n",
      "Episode 52600\tAverage Score: -0.00   Exploration: 0.22\n",
      "Episode 52700\tAverage Score: -0.00   Exploration: 0.22\n",
      "Episode 52800\tAverage Score: -0.00   Exploration: 0.22\n",
      "Episode 52900\tAverage Score: -0.00   Exploration: 0.22\n",
      "Episode 53000\tAverage Score: -0.00   Exploration: 0.22\n",
      "Episode 53100\tAverage Score: -0.00   Exploration: 0.22\n",
      "Episode 53200\tAverage Score: -0.00   Exploration: 0.22\n",
      "Episode 53300\tAverage Score: -0.00   Exploration: 0.22\n",
      "Episode 53400\tAverage Score: -0.00   Exploration: 0.22\n",
      "Episode 53500\tAverage Score: -0.00   Exploration: 0.22\n",
      "Episode 53600\tAverage Score: -0.00   Exploration: 0.22\n",
      "Episode 53700\tAverage Score: -0.00   Exploration: 0.21\n",
      "Episode 53800\tAverage Score: -0.00   Exploration: 0.21\n",
      "Episode 53900\tAverage Score: -0.00   Exploration: 0.21\n",
      "Episode 54000\tAverage Score: -0.00   Exploration: 0.21\n",
      "Episode 54100\tAverage Score: -0.00   Exploration: 0.21\n",
      "Episode 54200\tAverage Score: -0.00   Exploration: 0.21\n",
      "Episode 54300\tAverage Score: -0.00   Exploration: 0.21\n",
      "Episode 54400\tAverage Score: -0.00   Exploration: 0.21\n",
      "Episode 54500\tAverage Score: -0.00   Exploration: 0.21\n",
      "Episode 54600\tAverage Score: -0.00   Exploration: 0.21\n",
      "Episode 54700\tAverage Score: -0.00   Exploration: 0.21\n",
      "Episode 54800\tAverage Score: -0.00   Exploration: 0.21\n",
      "Episode 54900\tAverage Score: -0.00   Exploration: 0.21\n",
      "Episode 55000\tAverage Score: -0.00   Exploration: 0.21\n",
      "Episode 55100\tAverage Score: -0.00   Exploration: 0.21\n",
      "Episode 55200\tAverage Score: -0.00   Exploration: 0.21\n",
      "Episode 55300\tAverage Score: -0.00   Exploration: 0.21\n",
      "Episode 55400\tAverage Score: -0.00   Exploration: 0.20\n",
      "Episode 55500\tAverage Score: -0.00   Exploration: 0.20\n",
      "Episode 55600\tAverage Score: -0.00   Exploration: 0.20\n",
      "Episode 55700\tAverage Score: -0.00   Exploration: 0.20\n",
      "Episode 55800\tAverage Score: -0.00   Exploration: 0.20\n",
      "Episode 55900\tAverage Score: -0.00   Exploration: 0.20\n",
      "Episode 56000\tAverage Score: -0.00   Exploration: 0.20\n",
      "Episode 56100\tAverage Score: -0.00   Exploration: 0.20\n",
      "Episode 56200\tAverage Score: -0.00   Exploration: 0.20\n",
      "Episode 56300\tAverage Score: -0.00   Exploration: 0.20\n",
      "Episode 56400\tAverage Score: -0.00   Exploration: 0.20\n",
      "Episode 56500\tAverage Score: -0.00   Exploration: 0.20\n",
      "Episode 56600\tAverage Score: -0.00   Exploration: 0.20\n",
      "Episode 56700\tAverage Score: -0.00   Exploration: 0.20\n",
      "Episode 56800\tAverage Score: -0.00   Exploration: 0.20\n",
      "Episode 56900\tAverage Score: -0.00   Exploration: 0.20\n",
      "Episode 57000\tAverage Score: -0.00   Exploration: 0.20\n",
      "Episode 57100\tAverage Score: -0.00   Exploration: 0.19\n",
      "Episode 57200\tAverage Score: -0.00   Exploration: 0.19\n",
      "Episode 57300\tAverage Score: -0.00   Exploration: 0.19\n",
      "Episode 57400\tAverage Score: -0.00   Exploration: 0.19\n",
      "Episode 57500\tAverage Score: -0.00   Exploration: 0.19\n",
      "Episode 57600\tAverage Score: -0.00   Exploration: 0.19\n",
      "Episode 57700\tAverage Score: -0.00   Exploration: 0.19\n",
      "Episode 57800\tAverage Score: -0.00   Exploration: 0.19\n",
      "Episode 57900\tAverage Score: -0.00   Exploration: 0.19\n",
      "Episode 58000\tAverage Score: -0.00   Exploration: 0.19\n",
      "Episode 58100\tAverage Score: -0.00   Exploration: 0.19\n",
      "Episode 58200\tAverage Score: -0.00   Exploration: 0.19\n",
      "Episode 58300\tAverage Score: -0.00   Exploration: 0.19\n",
      "Episode 58400\tAverage Score: -0.00   Exploration: 0.19\n",
      "Episode 58500\tAverage Score: -0.00   Exploration: 0.19\n",
      "Episode 58600\tAverage Score: -0.00   Exploration: 0.19\n",
      "Episode 58700\tAverage Score: -0.00   Exploration: 0.19\n",
      "Episode 58800\tAverage Score: -0.00   Exploration: 0.19\n",
      "Episode 58900\tAverage Score: -0.00   Exploration: 0.19\n",
      "Episode 59000\tAverage Score: -0.00   Exploration: 0.18\n",
      "Episode 59100\tAverage Score: -0.00   Exploration: 0.18\n",
      "Episode 59200\tAverage Score: -0.00   Exploration: 0.18\n",
      "Episode 59300\tAverage Score: -0.00   Exploration: 0.18\n",
      "Episode 59400\tAverage Score: -0.00   Exploration: 0.18\n",
      "Episode 59500\tAverage Score: -0.00   Exploration: 0.18\n",
      "Episode 59600\tAverage Score: -0.00   Exploration: 0.18\n",
      "Episode 59700\tAverage Score: -0.00   Exploration: 0.18\n",
      "Episode 59800\tAverage Score: -0.00   Exploration: 0.18\n",
      "Episode 59900\tAverage Score: -0.00   Exploration: 0.18\n",
      "Episode 60000\tAverage Score: -0.00   Exploration: 0.18\n",
      "Episode 60100\tAverage Score: -0.00   Exploration: 0.18\n",
      "Episode 60200\tAverage Score: -0.00   Exploration: 0.18\n",
      "Episode 60300\tAverage Score: -0.00   Exploration: 0.18\n",
      "Episode 60400\tAverage Score: -0.00   Exploration: 0.18\n",
      "Episode 60500\tAverage Score: -0.00   Exploration: 0.18\n",
      "Episode 60600\tAverage Score: -0.00   Exploration: 0.18\n",
      "Episode 60700\tAverage Score: -0.00   Exploration: 0.18\n",
      "Episode 60800\tAverage Score: -0.00   Exploration: 0.18\n",
      "Episode 60900\tAverage Score: -0.00   Exploration: 0.18\n",
      "Episode 61000\tAverage Score: -0.00   Exploration: 0.17\n",
      "Episode 61100\tAverage Score: -0.00   Exploration: 0.17\n",
      "Episode 61200\tAverage Score: -0.00   Exploration: 0.17\n",
      "Episode 61300\tAverage Score: -0.00   Exploration: 0.17\n",
      "Episode 61400\tAverage Score: -0.00   Exploration: 0.17\n",
      "Episode 61500\tAverage Score: -0.00   Exploration: 0.17\n",
      "Episode 61600\tAverage Score: -0.00   Exploration: 0.17\n",
      "Episode 61700\tAverage Score: -0.00   Exploration: 0.17\n",
      "Episode 61800\tAverage Score: -0.00   Exploration: 0.17\n",
      "Episode 61900\tAverage Score: -0.00   Exploration: 0.17\n",
      "Episode 62000\tAverage Score: -0.00   Exploration: 0.17\n",
      "Episode 62100\tAverage Score: -0.00   Exploration: 0.17\n",
      "Episode 62200\tAverage Score: -0.00   Exploration: 0.17\n",
      "Episode 62300\tAverage Score: -0.00   Exploration: 0.17\n",
      "Episode 62400\tAverage Score: -0.00   Exploration: 0.17\n",
      "Episode 62500\tAverage Score: -0.00   Exploration: 0.17\n",
      "Episode 62600\tAverage Score: -0.00   Exploration: 0.17\n",
      "Episode 62700\tAverage Score: -0.00   Exploration: 0.17\n",
      "Episode 62800\tAverage Score: -0.00   Exploration: 0.17\n",
      "Episode 62900\tAverage Score: -0.00   Exploration: 0.17\n",
      "Episode 63000\tAverage Score: -0.00   Exploration: 0.16\n",
      "Episode 63100\tAverage Score: -0.00   Exploration: 0.16\n",
      "Episode 63200\tAverage Score: -0.00   Exploration: 0.16\n",
      "Episode 63300\tAverage Score: -0.00   Exploration: 0.16\n",
      "Episode 63400\tAverage Score: -0.00   Exploration: 0.16\n",
      "Episode 63500\tAverage Score: -0.00   Exploration: 0.16\n",
      "Episode 63600\tAverage Score: -0.00   Exploration: 0.16\n",
      "Episode 63700\tAverage Score: -0.00   Exploration: 0.16\n",
      "Episode 63800\tAverage Score: -0.00   Exploration: 0.16\n",
      "Episode 63900\tAverage Score: -0.00   Exploration: 0.16\n",
      "Episode 64000\tAverage Score: -0.00   Exploration: 0.16\n",
      "Episode 64100\tAverage Score: -0.00   Exploration: 0.16\n",
      "Episode 64200\tAverage Score: -0.00   Exploration: 0.16\n",
      "Episode 64300\tAverage Score: -0.00   Exploration: 0.16\n",
      "Episode 64400\tAverage Score: -0.00   Exploration: 0.16\n",
      "Episode 64500\tAverage Score: -0.00   Exploration: 0.16\n",
      "Episode 64600\tAverage Score: -0.00   Exploration: 0.16\n",
      "Episode 64700\tAverage Score: -0.00   Exploration: 0.16\n",
      "Episode 64800\tAverage Score: -0.00   Exploration: 0.16\n",
      "Episode 64900\tAverage Score: -0.00   Exploration: 0.16\n",
      "Episode 65000\tAverage Score: -0.00   Exploration: 0.16\n",
      "Episode 65100\tAverage Score: -0.00   Exploration: 0.16\n",
      "Episode 65200\tAverage Score: -0.00   Exploration: 0.15\n",
      "Episode 65300\tAverage Score: -0.00   Exploration: 0.15\n",
      "Episode 65400\tAverage Score: -0.00   Exploration: 0.15\n",
      "Episode 65500\tAverage Score: -0.00   Exploration: 0.15\n",
      "Episode 65600\tAverage Score: -0.00   Exploration: 0.15\n",
      "Episode 65700\tAverage Score: -0.00   Exploration: 0.15\n",
      "Episode 65800\tAverage Score: -0.00   Exploration: 0.15\n",
      "Episode 65900\tAverage Score: -0.00   Exploration: 0.15\n",
      "Episode 66000\tAverage Score: -0.00   Exploration: 0.15\n",
      "Episode 66100\tAverage Score: -0.00   Exploration: 0.15\n",
      "Episode 66200\tAverage Score: -0.00   Exploration: 0.15\n",
      "Episode 66300\tAverage Score: -0.00   Exploration: 0.15\n",
      "Episode 66400\tAverage Score: -0.00   Exploration: 0.15\n",
      "Episode 66500\tAverage Score: -0.00   Exploration: 0.15\n",
      "Episode 66600\tAverage Score: -0.00   Exploration: 0.15\n",
      "Episode 66700\tAverage Score: -0.00   Exploration: 0.15\n",
      "Episode 66800\tAverage Score: -0.00   Exploration: 0.15\n",
      "Episode 66900\tAverage Score: -0.00   Exploration: 0.15\n",
      "Episode 67000\tAverage Score: -0.00   Exploration: 0.15\n",
      "Episode 67100\tAverage Score: -0.00   Exploration: 0.15\n",
      "Episode 67200\tAverage Score: -0.00   Exploration: 0.15\n",
      "Episode 67300\tAverage Score: -0.00   Exploration: 0.15\n",
      "Episode 67400\tAverage Score: -0.00   Exploration: 0.15\n",
      "Episode 67500\tAverage Score: -0.00   Exploration: 0.15\n",
      "Episode 67600\tAverage Score: -0.00   Exploration: 0.14\n",
      "Episode 67700\tAverage Score: -0.00   Exploration: 0.14\n",
      "Episode 67800\tAverage Score: -0.00   Exploration: 0.14\n",
      "Episode 67900\tAverage Score: -0.00   Exploration: 0.14\n",
      "Episode 68000\tAverage Score: -0.00   Exploration: 0.14\n",
      "Episode 68100\tAverage Score: -0.00   Exploration: 0.14\n",
      "Episode 68200\tAverage Score: -0.00   Exploration: 0.14\n",
      "Episode 68300\tAverage Score: -0.00   Exploration: 0.14\n",
      "Episode 68400\tAverage Score: -0.00   Exploration: 0.14\n",
      "Episode 68500\tAverage Score: -0.00   Exploration: 0.14\n",
      "Episode 68600\tAverage Score: -0.00   Exploration: 0.14\n",
      "Episode 68700\tAverage Score: -0.00   Exploration: 0.14\n",
      "Episode 68800\tAverage Score: -0.00   Exploration: 0.14\n",
      "Episode 68900\tAverage Score: -0.00   Exploration: 0.14\n",
      "Episode 69000\tAverage Score: -0.00   Exploration: 0.14\n",
      "Episode 69100\tAverage Score: -0.00   Exploration: 0.14\n",
      "Episode 69200\tAverage Score: -0.00   Exploration: 0.14\n",
      "Episode 69300\tAverage Score: -0.00   Exploration: 0.14\n",
      "Episode 69400\tAverage Score: -0.00   Exploration: 0.14\n",
      "Episode 69500\tAverage Score: -0.00   Exploration: 0.14\n",
      "Episode 69600\tAverage Score: -0.00   Exploration: 0.14\n",
      "Episode 69700\tAverage Score: -0.00   Exploration: 0.14\n",
      "Episode 69800\tAverage Score: -0.00   Exploration: 0.14\n",
      "Episode 69900\tAverage Score: -0.00   Exploration: 0.14\n",
      "Episode 70000\tAverage Score: -0.00   Exploration: 0.14\n",
      "Episode 70100\tAverage Score: -0.00   Exploration: 0.13\n",
      "Episode 70200\tAverage Score: -0.00   Exploration: 0.13\n",
      "Episode 70300\tAverage Score: -0.00   Exploration: 0.13\n",
      "Episode 70400\tAverage Score: -0.00   Exploration: 0.13\n",
      "Episode 70500\tAverage Score: -0.00   Exploration: 0.13\n",
      "Episode 70600\tAverage Score: -0.00   Exploration: 0.13\n",
      "Episode 70700\tAverage Score: -0.00   Exploration: 0.13\n",
      "Episode 70800\tAverage Score: -0.00   Exploration: 0.13\n",
      "Episode 70900\tAverage Score: -0.00   Exploration: 0.13\n",
      "Episode 71000\tAverage Score: -0.00   Exploration: 0.13\n",
      "Episode 71100\tAverage Score: -0.00   Exploration: 0.13\n",
      "Episode 71200\tAverage Score: -0.00   Exploration: 0.13\n",
      "Episode 71300\tAverage Score: -0.00   Exploration: 0.13\n",
      "Episode 71400\tAverage Score: -0.00   Exploration: 0.13\n",
      "Episode 71500\tAverage Score: -0.00   Exploration: 0.13\n",
      "Episode 71600\tAverage Score: -0.00   Exploration: 0.13\n",
      "Episode 71700\tAverage Score: -0.00   Exploration: 0.13\n",
      "Episode 71800\tAverage Score: -0.00   Exploration: 0.13\n",
      "Episode 71900\tAverage Score: -0.00   Exploration: 0.13\n",
      "Episode 72000\tAverage Score: -0.00   Exploration: 0.13\n",
      "Episode 72100\tAverage Score: -0.00   Exploration: 0.13\n",
      "Episode 72200\tAverage Score: -0.00   Exploration: 0.13\n",
      "Episode 72300\tAverage Score: -0.00   Exploration: 0.13\n",
      "Episode 72400\tAverage Score: -0.00   Exploration: 0.13\n",
      "Episode 72500\tAverage Score: -0.00   Exploration: 0.13\n",
      "Episode 72600\tAverage Score: -0.00   Exploration: 0.13\n",
      "Episode 72700\tAverage Score: -0.00   Exploration: 0.13\n",
      "Episode 72800\tAverage Score: -0.00   Exploration: 0.12\n",
      "Episode 72900\tAverage Score: -0.00   Exploration: 0.12\n",
      "Episode 73000\tAverage Score: -0.00   Exploration: 0.12\n",
      "Episode 73100\tAverage Score: -0.00   Exploration: 0.12\n",
      "Episode 73200\tAverage Score: -0.00   Exploration: 0.12\n",
      "Episode 73300\tAverage Score: -0.00   Exploration: 0.12\n",
      "Episode 73400\tAverage Score: -0.00   Exploration: 0.12\n",
      "Episode 73500\tAverage Score: -0.00   Exploration: 0.12\n",
      "Episode 73600\tAverage Score: -0.00   Exploration: 0.12\n",
      "Episode 73700\tAverage Score: -0.00   Exploration: 0.12\n",
      "Episode 73800\tAverage Score: -0.00   Exploration: 0.12\n",
      "Episode 73900\tAverage Score: -0.00   Exploration: 0.12\n",
      "Episode 74000\tAverage Score: -0.00   Exploration: 0.12\n",
      "Episode 74100\tAverage Score: -0.00   Exploration: 0.12\n",
      "Episode 74200\tAverage Score: -0.00   Exploration: 0.12\n",
      "Episode 74300\tAverage Score: -0.00   Exploration: 0.12\n",
      "Episode 74400\tAverage Score: -0.00   Exploration: 0.12\n",
      "Episode 74500\tAverage Score: -0.00   Exploration: 0.12\n",
      "Episode 74600\tAverage Score: -0.00   Exploration: 0.12\n",
      "Episode 74700\tAverage Score: -0.00   Exploration: 0.12\n",
      "Episode 74800\tAverage Score: -0.00   Exploration: 0.12\n",
      "Episode 74900\tAverage Score: -0.00   Exploration: 0.12\n",
      "Episode 75000\tAverage Score: -0.00   Exploration: 0.12\n",
      "Episode 75100\tAverage Score: -0.00   Exploration: 0.12\n",
      "Episode 75200\tAverage Score: -0.00   Exploration: 0.12\n",
      "Episode 75300\tAverage Score: -0.00   Exploration: 0.12\n",
      "Episode 75400\tAverage Score: -0.00   Exploration: 0.12\n",
      "Episode 75500\tAverage Score: -0.00   Exploration: 0.12\n",
      "Episode 75600\tAverage Score: -0.00   Exploration: 0.12\n",
      "Episode 75700\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 75800\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 75900\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 76000\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 76100\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 76200\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 76300\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 76400\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 76500\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 76600\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 76700\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 76800\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 76900\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 77000\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 77100\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 77200\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 77300\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 77400\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 77500\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 77600\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 77700\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 77800\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 77900\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 78000\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 78100\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 78200\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 78300\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 78400\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 78500\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 78600\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 78700\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 78800\tAverage Score: -0.00   Exploration: 0.11\n",
      "Episode 78900\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 79000\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 79100\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 79200\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 79300\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 79400\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 79500\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 79600\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 79700\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 79800\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 79900\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 80000\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 80100\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 80200\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 80300\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 80400\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 80500\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 80600\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 80700\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 80800\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 80900\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 81000\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 81100\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 81200\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 81300\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 81400\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 81500\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 81600\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 81700\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 81800\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 81900\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 82000\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 82100\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 82200\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 82300\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 82400\tAverage Score: -0.00   Exploration: 0.10\n",
      "Episode 82500\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 82600\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 82700\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 82800\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 82900\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 83000\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 83100\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 83200\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 83300\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 83400\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 83500\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 83600\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 83700\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 83800\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 83900\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 84000\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 84100\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 84200\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 84300\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 84400\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 84500\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 84600\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 84700\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 84800\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 84900\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 85000\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 85100\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 85200\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 85300\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 85400\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 85500\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 85600\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 85700\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 85800\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 85900\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 86000\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 86100\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 86200\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 86300\tAverage Score: -0.00   Exploration: 0.09\n",
      "Episode 86400\tAverage Score: -0.00   Exploration: 0.08\n",
      "Episode 86500\tAverage Score: -0.00   Exploration: 0.08\n",
      "Episode 86600\tAverage Score: -0.00   Exploration: 0.08\n",
      "Episode 86700\tAverage Score: -0.00   Exploration: 0.08\n",
      "Episode 86800\tAverage Score: -0.00   Exploration: 0.08\n",
      "Episode 86900\tAverage Score: -0.00   Exploration: 0.08\n",
      "Episode 87000\tAverage Score: -0.00   Exploration: 0.08\n",
      "Episode 87100\tAverage Score: -0.00   Exploration: 0.08\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-85eb7693488e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# plot the scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m111\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-d49c62324902>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(n_iterations, gamma, print_every)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi_iteration\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iterations\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mscores_deque\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-d49c62324902>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(agent, env)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0menv_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mnext_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector_observations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\drlnd\\lib\\site-packages\\unityagents\\environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, vector_action, memory, text_action)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m             outputs = self.communicator.exchange(\n\u001b[1;32m--> 369\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_generate_step_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector_action\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    370\u001b[0m             )\n\u001b[0;32m    371\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\drlnd\\lib\\site-packages\\unityagents\\rpc_communicator.py\u001b[0m in \u001b[0;36mexchange\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mmessage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mmessage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munity_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\drlnd\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_writable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ForkingPickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrecv_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\drlnd\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36m_send_bytes\u001b[1;34m(self, buf)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_send_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m             \u001b[0mov\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_winapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWriteFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverlapped\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_winapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mERROR_IO_PENDING\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = train()  \n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.savefig(\"maddpg.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'action_size' and 'random_seed'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-de674a4d824f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMaddpgAgent\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_local\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'checkpoint_actor.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_local\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'checkpoint_critic.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m                                      \u001b[1;31m# play game for 5 episodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'action_size' and 'random_seed'"
     ]
    }
   ],
   "source": [
    "agent = MaddpgAgent( state_size, action_size)\n",
    "agent.load()\n",
    "\n",
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        \n",
    "        actions = agent.act(states, add_noise=False)       # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}